{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Feature Extraction with Python\n",
    "\n",
    "Library used for feature extraction: [skimage](http://scikit-image.org/) \n",
    "\n",
    "Dataset used: [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torchvision import datasets\n",
    "import PIL\n",
    "from skimage.feature import local_binary_pattern, greycomatrix, greycoprops\n",
    "from skimage.filters import gabor\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [    
    "trainDset =  datasets.CIFAR10('./cifar10/', train=True, download=True) \n",
    "testDset =  datasets.CIFAR10('./cifar10/', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of train and test datasets\n",
    "print('No. of samples in train set: '+str(len(trainDset)))\n",
    "print('No. of samples in test set: '+str(len(testDset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction on single image\n",
    "img = trainDset[0][0] #PIL image\n",
    "img_gray = img.convert('L') #Converting to grayscale\n",
    "img_arr = np.array(img_gray) #Converting to array\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding LBP\n",
    "feat_lbp = local_binary_pattern(img_arr,8,1,'uniform') #Radius = 1, No. of neighbours = 8\n",
    "feat_lbp = np.uint8((feat_lbp/feat_lbp.max())*255) #Converting to unit8\n",
    "lbp_img = PIL.Image.fromarray(feat_lbp) #Conversion from array to PIL image\n",
    "plt.imshow(lbp_img,cmap='gray') #Displaying LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy and Entropy of LBP feature\n",
    "lbp_hist,_ = np.histogram(feat_lbp,8)\n",
    "lbp_hist = np.array(lbp_hist,dtype=float)\n",
    "lbp_prob = np.divide(lbp_hist,np.sum(lbp_hist))\n",
    "lbp_energy = np.sum(lbp_prob**2)\n",
    "lbp_entropy = -np.sum(np.multiply(lbp_prob,np.log2(lbp_prob)))\n",
    "print('LBP energy = '+str(lbp_energy))\n",
    "print('LBP entropy = '+str(lbp_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding GLCM features from co-occurance matrix\n",
    "gCoMat = greycomatrix(img_arr, [2], [0],256,symmetric=True, normed=True) # Co-occurance matrix\n",
    "contrast = greycoprops(gCoMat, prop='contrast')\n",
    "dissimilarity = greycoprops(gCoMat, prop='dissimilarity')\n",
    "homogeneity = greycoprops(gCoMat, prop='homogeneity')\n",
    "energy = greycoprops(gCoMat, prop='energy')\n",
    "correlation = greycoprops(gCoMat, prop='correlation')\n",
    "print('Contrast = '+str(contrast[0][0]))\n",
    "print('Dissimilarity = '+str(dissimilarity[0][0]))\n",
    "print('Homogeneity = '+str(homogeneity[0][0]))\n",
    "print('Energy = '+str(energy[0][0]))\n",
    "print('Correlation = '+str(correlation[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabor filter\n",
    "gaborFilt_real,gaborFilt_imag = gabor(img_arr,frequency=0.6)\n",
    "gaborFilt = (gaborFilt_real**2+gaborFilt_imag**2)//2\n",
    "# Displaying the filter response\n",
    "fig, ax = plt.subplots(1,3)    \n",
    "ax[0].imshow(gaborFilt_real,cmap='gray')\n",
    "ax[1].imshow(gaborFilt_imag,cmap='gray')\n",
    "ax[2].imshow(gaborFilt,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy and Entropy of Gabor filter response\n",
    "gabor_hist,_ = np.histogram(gaborFilt,8)\n",
    "gabor_hist = np.array(gabor_hist,dtype=float)\n",
    "gabor_prob = np.divide(gabor_hist,np.sum(gabor_hist))\n",
    "gabor_energy = np.sum(gabor_prob**2)\n",
    "gabor_entropy = -np.sum(np.multiply(gabor_prob,np.log2(gabor_prob)))\n",
    "print('Gabor energy = '+str(gabor_energy))\n",
    "print('Gabor entropy = '+str(gabor_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from all images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating training data by extracting features from all images\n",
    "label = []\n",
    "featLength = 2+5+2\n",
    "trainFeats = np.zeros((len(trainDset),featLength)) #Feature vector of each image is of size 1x1030\n",
    "for tr in range(len(trainDset)):\n",
    "    print(str(tr+1)+'/'+str(len(trainDset)))\n",
    "    img = trainDset[tr][0] #One image at a time\n",
    "    img_gray = img.convert('L') #Converting to grayscale\n",
    "    img_arr = np.array(img_gray.getdata()).reshape(img.size[1],img.size[0]) #Converting to array\n",
    "    # LBP\n",
    "    feat_lbp = local_binary_pattern(img_arr,5,2,'uniform').reshape(img.size[0]*img.size[1])\n",
    "    lbp_hist,_ = np.histogram(feat_lbp,8)\n",
    "    lbp_hist = np.array(lbp_hist,dtype=float)\n",
    "    lbp_prob = np.divide(lbp_hist,np.sum(lbp_hist))\n",
    "    lbp_energy = np.nansum(lbp_prob**2)\n",
    "    lbp_entropy = -np.nansum(np.multiply(lbp_prob,np.log2(lbp_prob)))   \n",
    "    # GLCM\n",
    "    gCoMat = greycomatrix(img_arr, [2], [0],256,symmetric=True, normed=True)\n",
    "    contrast = greycoprops(gCoMat, prop='contrast')\n",
    "    dissimilarity = greycoprops(gCoMat, prop='dissimilarity')\n",
    "    homogeneity = greycoprops(gCoMat, prop='homogeneity')    \n",
    "    energy = greycoprops(gCoMat, prop='energy')\n",
    "    correlation = greycoprops(gCoMat, prop='correlation')    \n",
    "    feat_glcm = np.array([contrast[0][0],dissimilarity[0][0],homogeneity[0][0],energy[0][0],correlation[0][0]])\n",
    "    # Gabor filter\n",
    "    gaborFilt_real,gaborFilt_imag = gabor(img_arr,frequency=0.6)\n",
    "    gaborFilt = (gaborFilt_real**2+gaborFilt_imag**2)//2\n",
    "    gabor_hist,_ = np.histogram(gaborFilt,8)\n",
    "    gabor_hist = np.array(gabor_hist,dtype=float)\n",
    "    gabor_prob = np.divide(gabor_hist,np.sum(gabor_hist))\n",
    "    gabor_energy = np.nansum(gabor_prob**2)\n",
    "    gabor_entropy = -np.nansum(np.multiply(gabor_prob,np.log2(gabor_prob)))\n",
    "    # Concatenating features(2+5+2)    \n",
    "    concat_feat = np.concatenate(([lbp_energy,lbp_entropy],feat_glcm,[gabor_energy,gabor_entropy]),axis=0)\n",
    "    trainFeats[tr,:] = concat_feat #Stacking features vectors for each image\n",
    "    # Class label\n",
    "    label.append(trainDset[tr][1])\n",
    "trainLabel = np.array(label) #Conversion from list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating testing data by extracting features from all images\n",
    "label = []\n",
    "testFeats = np.zeros((len(testDset),featLength)) #Feature vector of each image is of size 1x1030\n",
    "for ts in range(len(testDset)):\n",
    "    print(str(ts+1)+'/'+str(len(testDset)))\n",
    "    img = testDset[ts][0] #One image at a time\n",
    "    img_gray = img.convert('L') #Converting to grayscale\n",
    "    img_arr = np.array(img_gray.getdata()).reshape(img.size[1],img.size[0]) #Converting to array\n",
    "    # LBP\n",
    "    feat_lbp = local_binary_pattern(img_arr,5,2,'uniform').reshape(img.size[0]*img.size[1])\n",
    "    lbp_hist,_ = np.histogram(feat_lbp,8)\n",
    "    lbp_hist = np.array(lbp_hist,dtype=float)\n",
    "    lbp_prob = np.divide(lbp_hist,np.sum(lbp_hist))\n",
    "    lbp_energy = np.nansum(lbp_prob**2)\n",
    "    lbp_entropy = -np.nansum(np.multiply(lbp_prob,np.log2(lbp_prob)))\n",
    "    # GLCM\n",
    "    gCoMat = greycomatrix(img_arr, [2], [0],256,symmetric=True, normed=True)\n",
    "    contrast = greycoprops(gCoMat, prop='contrast')\n",
    "    dissimilarity = greycoprops(gCoMat, prop='dissimilarity')\n",
    "    homogeneity = greycoprops(gCoMat, prop='homogeneity')    \n",
    "    energy = greycoprops(gCoMat, prop='energy')\n",
    "    correlation = greycoprops(gCoMat, prop='correlation')    \n",
    "    feat_glcm = np.array([contrast[0][0],dissimilarity[0][0],homogeneity[0][0],energy[0][0],correlation[0][0]])\n",
    "    # Gabor filter\n",
    "    gaborFilt_real,gaborFilt_imag = gabor(img_arr,frequency=0.6)\n",
    "    gaborFilt = (gaborFilt_real**2+gaborFilt_imag**2)//2\n",
    "    gabor_hist,_ = np.histogram(gaborFilt,8)\n",
    "    gabor_hist = np.array(gabor_hist,dtype=float)\n",
    "    gabor_prob = np.divide(gabor_hist,np.sum(gabor_hist))\n",
    "    gabor_energy = np.nansum(gabor_prob**2)\n",
    "    gabor_entropy = -np.nansum(np.multiply(gabor_prob,np.log2(gabor_prob)))\n",
    "    # Concatenating features(2+5+2)    \n",
    "    concat_feat = np.concatenate(([lbp_energy,lbp_entropy],feat_glcm,[gabor_energy,gabor_entropy]),axis=0)    \n",
    "    testFeats[ts,:] = concat_feat  #Stacking features vectors for each image\n",
    "    # Class label\n",
    "    label.append(testDset[ts][1])\n",
    "testLabel = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the train features to the range [0,1]\n",
    "trMaxs = np.amax(trainFeats,axis=0) #Finding maximum along each column\n",
    "trMins = np.amin(trainFeats,axis=0) #Finding maximum along each column\n",
    "trMaxs_rep = np.tile(trMaxs,(50000,1)) #Repeating the maximum value along the rows\n",
    "trMins_rep = np.tile(trMins,(50000,1)) #Repeating the minimum value along the rows\n",
    "trainFeatsNorm = np.divide(trainFeats-trMins_rep,trMaxs_rep) #Element-wise division\n",
    "# Normalizing the test features\n",
    "tsMaxs_rep = np.tile(trMaxs,(10000,1)) #Repeating the maximum value along the rows\n",
    "tsMins_rep = np.tile(trMins,(10000,1)) #Repeating the maximum value along the rows\n",
    "testFeatsNorm = np.divide(testFeats-tsMins_rep,tsMaxs_rep) #Element-wise division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving feature matrices to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving normalized training data and labels\n",
    "with open(\"trainFeats.pckl\", \"wb\") as f:\n",
    "    pickle.dump(trainFeatsNorm, f)\n",
    "with open(\"trainLabel.pckl\", \"wb\") as f:\n",
    "    pickle.dump(trainLabel, f)\n",
    "    \n",
    "# Saving normalized testing data and labels\n",
    "with open(\"testFeats.pckl\", \"wb\") as f:\n",
    "    pickle.dump(testFeatsNorm, f)\n",
    "with open(\"testLabel.pckl\", \"wb\") as f:\n",
    "    pickle.dump(testLabel, f)\n",
    "    \n",
    "print('Files saved to disk!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
