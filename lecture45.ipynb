{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 45: Transfer Learning ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms,datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transform = transforms.Compose([transforms.Scale(224),transforms.ToTensor()])\n",
    "BatchSize = 32\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./CIFAR10', train=True, download=True, transform=apply_transform)\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
    "                                          shuffle=True, num_workers=4) # Creating dataloader\n",
    "\n",
    "testset = datasets.CIFAR10(root='./CIFAR10', train=False, download=True, transform=apply_transform)\n",
    "testLoader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
    "                                         shuffle=False, num_workers=4) # Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of train and test datasets\n",
    "print('No. of samples in train set: '+str(len(trainLoader.dataset)))\n",
    "print('No. of samples in test set: '+str(len(testLoader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1 = models.resnet18() # Training from scratch\n",
    "net2 = models.resnet18(pretrained=True) # End-to-end fine-tuning\n",
    "net3 = models.resnet18(pretrained=True) # Training only the last layer\n",
    "print(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counting number of trainable parameters\n",
    "totalParams = 0\n",
    "for params in net1.parameters():\n",
    "    print(params.size())\n",
    "    totalParams += np.sum(np.prod(params.size()))\n",
    "print('Total number of parameters: '+str(totalParams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the last fully-connected layer for 10 classes\n",
    "net1.fc = nn.Linear(512,10)\n",
    "net2.fc = nn.Linear(512,10)\n",
    "net3.fc = nn.Linear(512,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying initial weights for visualization\n",
    "# Model 1\n",
    "init_weightConv1_1 = copy.deepcopy(net1.conv1.weight.data) # 1st conv layer\n",
    "init_weightConv2_1 = copy.deepcopy(net1.layer1[0].conv1.weight.data) # 2nd conv layer\n",
    "# Model 2\n",
    "init_weightConv1_2 = copy.deepcopy(net2.conv1.weight.data) # 1st conv layer\n",
    "init_weightConv2_2 = copy.deepcopy(net2.layer1[0].conv1.weight.data) # 2nd conv layer\n",
    "# Model 3\n",
    "init_weightConv1_3 = copy.deepcopy(net3.conv1.weight.data) # 1st conv layer\n",
    "init_weightConv2_3 = copy.deepcopy(net3.layer1[0].conv1.weight.data) # 2nd conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')   \n",
    "    net1 = net1.cuda()\n",
    "    net2 = net2.cuda()\n",
    "    net3 = net3.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # Negative Log-likelihood\n",
    "optimizer1 = optim.Adam(net1.parameters(), lr=1e-4) # Adam; passing all params\n",
    "optimizer2 = optim.Adam(net2.parameters(), lr=1e-4) # Adam; passing all params\n",
    "optimizer3 = optim.Adam(net3.fc.parameters(), lr=1e-4) # Adam; passing params of only the last fc layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "trainLoss1 = []\n",
    "trainAuxLoss1 = []\n",
    "trainTotalLoss1 = []\n",
    "trainAcc1 = []\n",
    "testLoss1 = []\n",
    "testAcc1 = []\n",
    "\n",
    "trainLoss2 = []\n",
    "trainAuxLoss2 = []\n",
    "trainTotalLoss2 = []\n",
    "trainAcc2 = []\n",
    "testLoss2 = []\n",
    "testAcc2 = []\n",
    "\n",
    "trainLoss3 = []\n",
    "trainAuxLoss3 = []\n",
    "trainTotalLoss3 = []\n",
    "trainAcc3 = []\n",
    "testLoss3 = []\n",
    "testAcc3 = []\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(iterations):\n",
    "    epochStart = time.time()\n",
    "    # Model 1\n",
    "    runningLoss1 = 0.0   \n",
    "    avgTotalLoss1 = 0.0\n",
    "    running_correct1 = 0\n",
    "    # Model 2\n",
    "    runningLoss2 = 0.0   \n",
    "    avgTotalLoss2 = 0.0\n",
    "    running_correct2 = 0\n",
    "    # Model 3\n",
    "    runningLoss3 = 0.0   \n",
    "    avgTotalLoss3 = 0.0\n",
    "    running_correct3 = 0\n",
    "    \n",
    "    net1.train(True) # For training   \n",
    "    net2.train(True) # For training   \n",
    "    net3.train(True) # For training   \n",
    "    for data in trainLoader:\n",
    "        inputs,labels = data\n",
    "        # Wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            # Model 1\n",
    "            outputs1 = net1(inputs)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)            \n",
    "            running_correct1 += (predicted1.cpu() == labels.data.cpu()).sum()\n",
    "            # Model 2\n",
    "            outputs2 = net2(inputs)\n",
    "            _, predicted2 = torch.max(outputs2.data, 1)            \n",
    "            running_correct2 += (predicted2.cpu() == labels.data.cpu()).sum()\n",
    "            # Model 3\n",
    "            outputs3 = net3(inputs)\n",
    "            _, predicted3 = torch.max(outputs3.data, 1)            \n",
    "            running_correct3 += (predicted3.cpu() == labels.data.cpu()).sum()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)  \n",
    "            # Model 1\n",
    "            outputs1 = net1(inputs)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            running_correct1 += (predicted1 == labels.data).sum()\n",
    "            # Model 2\n",
    "            outputs2 = net2(inputs)\n",
    "            _, predicted2 = torch.max(outputs2.data, 1)\n",
    "            running_correct2 += (predicted2 == labels.data).sum()\n",
    "            # Model 3\n",
    "            outputs3 = net3(inputs)\n",
    "            _, predicted3 = torch.max(outputs3.data, 1)\n",
    "            running_correct3 += (predicted3 == labels.data).sum()\n",
    "       \n",
    "        # Initialize gradients to zero\n",
    "        optimizer1.zero_grad()               \n",
    "        optimizer2.zero_grad()               \n",
    "        optimizer3.zero_grad()               \n",
    "        \n",
    "        # Compute loss/error\n",
    "        loss1 = criterion(F.log_softmax(outputs1), labels)\n",
    "        loss2 = criterion(F.log_softmax(outputs2), labels)\n",
    "        loss3 = criterion(F.log_softmax(outputs3), labels)\n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        loss3.backward()\n",
    "        # Update the network parameters\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()\n",
    "        # Accumulate loss per batch\n",
    "        runningLoss1 += loss1.data[0]         \n",
    "        runningLoss2 += loss2.data[0]         \n",
    "        runningLoss3 += loss3.data[0]         \n",
    "    avgTrainAcc1 = running_correct1/50000.0\n",
    "    avgTrainLoss1 = runningLoss1/50000.0    \n",
    "    trainAcc1.append(avgTrainAcc1)\n",
    "    trainLoss1.append(avgTrainLoss1)    \n",
    "    #-------------------------------\n",
    "    avgTrainAcc2 = running_correct2/50000.0\n",
    "    avgTrainLoss2 = runningLoss2/50000.0    \n",
    "    trainAcc2.append(avgTrainAcc2)\n",
    "    trainLoss2.append(avgTrainLoss2)    \n",
    "    #-------------------------------\n",
    "    avgTrainAcc3 = running_correct3/50000.0\n",
    "    avgTrainLoss3 = runningLoss3/50000.0    \n",
    "    trainAcc3.append(avgTrainAcc3)\n",
    "    trainLoss3.append(avgTrainLoss3)\n",
    "    \n",
    "    # Evaluating performance on test set for each epoch\n",
    "    net1.train(False) # For testing [Affects batch-norm and dropout layers (if any)]\n",
    "    net2.train(False)\n",
    "    net3.train(False)\n",
    "    running_correct1 = 0  \n",
    "    running_correct2 = 0  \n",
    "    running_correct3 = 0  \n",
    "    for data in testLoader:\n",
    "        inputs,labels = data\n",
    "        # Wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels= Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            # Model 1\n",
    "            outputs1 = net1(inputs)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)            \n",
    "            running_correct1 += (predicted1.cpu() == labels.data.cpu()).sum()\n",
    "            # Model 2\n",
    "            outputs2 = net2(inputs)\n",
    "            _, predicted2 = torch.max(outputs2.data, 1)            \n",
    "            running_correct2 += (predicted2.cpu() == labels.data.cpu()).sum()\n",
    "            # Model 3\n",
    "            outputs3 = net3(inputs)\n",
    "            _, predicted3 = torch.max(outputs3.data, 1)            \n",
    "            running_correct3 += (predicted3.cpu() == labels.data.cpu()).sum()\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            # Model 1\n",
    "            outputs1 = net1(inputs)\n",
    "            _, predicted1 = torch.max(outputs1.data, 1)\n",
    "            running_correct1 += (predicted1 == labels.data).sum()\n",
    "            # Model 2\n",
    "            outputs2 = net2(inputs)\n",
    "            _, predicted2 = torch.max(outputs2.data, 1)\n",
    "            running_correct2 += (predicted2 == labels.data).sum()\n",
    "            # Model 3\n",
    "            outputs3 = net3(inputs)\n",
    "            _, predicted3 = torch.max(outputs3.data, 1)\n",
    "            running_correct3 += (predicted3 == labels.data).sum()\n",
    "        \n",
    "        loss1 = criterion(F.log_softmax(outputs1), labels)\n",
    "        loss2 = criterion(F.log_softmax(outputs2), labels)\n",
    "        loss3 = criterion(F.log_softmax(outputs3), labels)\n",
    "        \n",
    "        runningLoss1 += loss1.data[0]         \n",
    "        runningLoss2 += loss2.data[0]         \n",
    "        runningLoss3 += loss3.data[0]         \n",
    "    avgTestLoss1 = runningLoss1/10000.0\n",
    "    avgTestAcc1 = running_correct1/10000.0\n",
    "    testLoss1.append(avgTestLoss1)\n",
    "    testAcc1.append(avgTestAcc1)\n",
    "    #---------------------------\n",
    "    avgTestLoss2 = runningLoss2/10000.0\n",
    "    avgTestAcc2 = running_correct2/10000.0\n",
    "    testLoss2.append(avgTestLoss2)\n",
    "    testAcc2.append(avgTestAcc2)\n",
    "    #---------------------------\n",
    "    avgTestLoss3 = runningLoss3/10000.0\n",
    "    avgTestAcc3 = running_correct3/10000.0\n",
    "    testLoss3.append(avgTestLoss3)\n",
    "    testAcc3.append(avgTestAcc3)\n",
    "        \n",
    "    \n",
    "    # Plotting training loss vs Epochs\n",
    "    fig1 = plt.figure(1)        \n",
    "    plt.plot(range(epoch+1),trainLoss1,'r-',label='train')  \n",
    "    plt.plot(range(epoch+1),testLoss1,'g-',label='test') \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss of Model 1')   \n",
    "    # Plotting testing accuracy vs Epochs\n",
    "    fig2 = plt.figure(2)        \n",
    "    plt.plot(range(epoch+1),trainAcc1,'r-',label='train')    \n",
    "    plt.plot(range(epoch+1),testAcc1,'g-',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy of Model 1')   \n",
    "        \n",
    "    # Plotting training loss vs Epochs\n",
    "    fig3 = plt.figure(3)        \n",
    "    plt.plot(range(epoch+1),trainLoss2,'r-',label='train')  \n",
    "    plt.plot(range(epoch+1),testLoss2,'g-',label='test') \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss of Model 2')   \n",
    "    # Plotting testing accuracy vs Epochs\n",
    "    fig4 = plt.figure(4)        \n",
    "    plt.plot(range(epoch+1),trainAcc2,'r-',label='train')    \n",
    "    plt.plot(range(epoch+1),testAcc2,'g-',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy of Model 2')\n",
    "        \n",
    "    # Plotting training loss vs Epochs\n",
    "    fig5 = plt.figure(5)        \n",
    "    plt.plot(range(epoch+1),trainLoss3,'r-',label='train')  \n",
    "    plt.plot(range(epoch+1),testLoss3,'g-',label='test') \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss of Model 1')   \n",
    "    # Plotting testing accuracy vs Epochs\n",
    "    fig6 = plt.figure(6)        \n",
    "    plt.plot(range(epoch+1),trainAcc3,'r-',label='train')    \n",
    "    plt.plot(range(epoch+1),testAcc3,'g-',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy of Model 1')\n",
    "        \n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('Iteration: {:.0f} /{:.0f} Model 1  ;  Training Loss: {:.6f} ; Testing Acc: {:.3f}'\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss1,avgTestAcc1*100))\n",
    "    print('Iteration: {:.0f} /{:.0f} Model 2  ;  Training Loss: {:.6f} ; Testing Acc: {:.3f} '\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss2,avgTestAcc2*100))\n",
    "    print('Iteration: {:.0f} /{:.0f} Model 3  ;  Training Loss: {:.6f} ; Testing Acc: {:.3f} '\\\n",
    "      .format(epoch + 1,iterations,avgTrainLoss3,avgTestAcc3*100))\n",
    "    print('Time consumed: {:.0f}m {:.0f}s'.format(epochEnd//60,epochEnd%60))\n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting training loss vs Epochs\n",
    "fig7 = plt.figure(7)        \n",
    "plt.plot(range(epoch+1),trainLoss1,'r-',label='model1')  \n",
    "plt.plot(range(epoch+1),trainLoss2,'g-',label='model2') \n",
    "plt.plot(range(epoch+1),trainLoss3,'b-',label='model3') \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Train Loss')  \n",
    "\n",
    "fig8 = plt.figure(8)        \n",
    "plt.plot(range(epoch+1),testLoss1,'r-',label='model1')  \n",
    "plt.plot(range(epoch+1),testLoss2,'g-',label='model2') \n",
    "plt.plot(range(epoch+1),testLoss3,'b-',label='model3') \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss') \n",
    "\n",
    "fig9 = plt.figure(9)        \n",
    "plt.plot(range(epoch+1),testAcc1,'r-',label='model1')  \n",
    "plt.plot(range(epoch+1),testAcc2,'g-',label='model2') \n",
    "plt.plot(range(epoch+1),testAcc3,'b-',label='model3') \n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying trained weights for visualization\n",
    "if use_gpu:\n",
    "    trained_weightConv1_1 = copy.deepcopy(net1.conv1.weight.data.cpu())\n",
    "    trained_weightConv2_1 = copy.deepcopy(net1.layer1[0].conv1.weight.data.cpu())\n",
    "    \n",
    "    trained_weightConv1_2 = copy.deepcopy(net2.conv1.weight.data.cpu())\n",
    "    trained_weightConv2_2 = copy.deepcopy(net2.layer1[0].conv1.weight.data.cpu())\n",
    "    \n",
    "    trained_weightConv1_3 = copy.deepcopy(net3.conv1.weight.data.cpu())\n",
    "    trained_weightConv2_3 = copy.deepcopy(net3.layer1[0].conv1.weight.data.cpu())\n",
    "else:\n",
    "    trained_weightConv1_1 = copy.deepcopy(net1.conv1.weight.data)\n",
    "    trained_weightConv2_1 = copy.deepcopy(net1.layer1[0].conv1.weight.data)\n",
    "    \n",
    "    trained_weightConv1_2 = copy.deepcopy(net2.conv1.weight.data)\n",
    "    trained_weightConv2_2 = copy.deepcopy(net2.layer1[0].conv1.weight.data)\n",
    "    \n",
    "    trained_weightConv1_3 = copy.deepcopy(net3.conv1.weight.data)\n",
    "    trained_weightConv2_3 = copy.deepcopy(net3.layer1[0].conv1.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img, strlabel):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.abs(npimg)\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 10\n",
    "    fig_size[1] = 10\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.figure()\n",
    "    plt.title(strlabel)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 1st convolutional layer of Model 1\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_1,nrow=8,normalize=True),'Initial weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv1_1,nrow=8,normalize=True),'Trained weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_1-trained_weightConv1_1,nrow=8,normalize=True),'Difference of weights: conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 2nd convolutional layer of Model 1\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_1[0].unsqueeze(1),nrow=8,normalize=True),'Initial weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv2_1[0].unsqueeze(1),nrow=8,normalize=True),'Trained weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_1[0].unsqueeze(1)-trained_weightConv2_1[0].unsqueeze(1),nrow=8,normalize=True),'Difference of weights: conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 1st convolutional layer of Model 2\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_2,nrow=8,normalize=True),'Initial weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv1_2,nrow=8,normalize=True),'Trained weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_2-trained_weightConv1_2,nrow=8,normalize=True),'Difference of weights: conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 2nd convolutional layer of Model 2\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_2[0].unsqueeze(1),nrow=8,normalize=True),'Initial weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv2_2[0].unsqueeze(1),nrow=8,normalize=True),'Trained weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_2[0].unsqueeze(1)-trained_weightConv2_2[0].unsqueeze(1),nrow=8,normalize=True),'Difference of weights: conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 1st convolutional layer of Model 3\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_3,nrow=8,normalize=True),'Initial weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv1_3,nrow=8,normalize=True),'Trained weights: conv1')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv1_3-trained_weightConv1_3,nrow=8,normalize=True),'Difference of weights: conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing weights of 2nd convolutional layer of Model 3\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_3[0].unsqueeze(1),nrow=8,normalize=True),'Initial weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(trained_weightConv2_3[0].unsqueeze(1),nrow=8,normalize=True),'Trained weights: conv2')\n",
    "imshow(torchvision.utils.make_grid(init_weightConv2_3[0].unsqueeze(1)-trained_weightConv2_3[0].unsqueeze(1),nrow=8,normalize=True),'Difference of weights: conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
